# AI èƒ½åŠ›æ‰©å±•
### å½“å‰é—®é¢˜ï¼š
ç³»ç»ŸAIèƒ½åŠ›å±€é™äºå¯¹è´¦åˆ†æåœºæ™¯ï¼Œæ¨¡å‹å“åº”é€Ÿåº¦è¾ƒæ…¢ï¼Œæç¤ºè¯å·¥ç¨‹ä¸å¤Ÿä¼˜åŒ–ï¼Œç¼ºå°‘å¤šæ¨¡æ€æ”¯æŒå’Œé«˜çº§åˆ†æåŠŸèƒ½ã€‚
### æ”¹è¿›å»ºè®®ï¼š
1. æ‰©å±•AIåº”ç”¨åœºæ™¯ï¼ˆç”¨æˆ·è¡Œä¸ºåˆ†æã€å¼‚å¸¸æ£€æµ‹ã€æ™ºèƒ½æ¨èï¼‰
2. ä¼˜åŒ–AIæ¨¡å‹åŠ è½½å’Œè°ƒç”¨æµç¨‹ï¼Œæå‡å“åº”é€Ÿåº¦
3. å¼•å…¥æ‰¹é‡å¤„ç†å’Œä»»åŠ¡é˜Ÿåˆ—æœºåˆ¶
4. ä¼˜åŒ–æç¤ºè¯å·¥ç¨‹ï¼Œå‡å°‘ Token æ¶ˆè€—ï¼Œæå‡å‡†ç¡®æ€§
5. å®ç°å¤šæ¨¡æ€å’Œæ–‡æ¡£åˆ†æèƒ½åŠ›
## ğŸ—‚ï¸ å®Œæ•´çš„æ–‡ä»¶ç»“æ„å»ºè®®
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
src/services/ai/
â”œâ”€â”€ base.service.ts              # åŸºç¡€AIæœåŠ¡ï¼ˆå·²æä¾›ï¼‰
â”œâ”€â”€ user-behavior.service.ts     # ç”¨æˆ·è¡Œä¸ºåˆ†ææœåŠ¡ï¼ˆå·²æä¾›ï¼‰
â”œâ”€â”€ queue.service.ts             # AIä»»åŠ¡é˜Ÿåˆ—æœåŠ¡ï¼ˆå·²æä¾›ï¼‰
â”œâ”€â”€ document-recommendation.service.ts  # æ–‡æ¡£åˆ†æå’Œæ¨èæœåŠ¡ï¼ˆå½“å‰æ–‡ä»¶ï¼‰
â”œâ”€â”€ index.ts                     # ç»Ÿä¸€å¯¼å‡º
â””â”€â”€ types/
    â”œâ”€â”€ ai-task.types.ts         # AIä»»åŠ¡ç›¸å…³ç±»å‹
    â”œâ”€â”€ document.types.ts        # æ–‡æ¡£åˆ†æç±»å‹
    â””â”€â”€ recommendation.types.ts  # æ¨èæœåŠ¡ç±»å‹
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
src/services/ai/types/ai-task.types.ts
// AIä»»åŠ¡ç›¸å…³ç±»å‹å®šä¹‰
export type AITaskType = 
  | 'user_behavior_analysis' 
  | 'churn_prediction' 
  | 'document_analysis'
  | 'content_generation'
  | 'data_processing'
  | 'sentiment_analysis'
  | 'recommendation'
  | 'custom';

export interface AITask {
  id: string;
  type: AITaskType;
  data: any;
  metadata?: {
    userId?: string;
    projectId?: string;
    priority?: 'low' | 'normal' | 'high' | 'critical';
    tags?: string[];
    source?: string;
  };
  options?: {
    priority?: number;
    attempts?: number;
    timeout?: number;
    delay?: number;
  };
  callback?: {
    url?: string;
    event?: string;
    webhook?: string;
    headers?: Record<string, string>;
  };
}

export interface AITaskResult {
  id: string;
  result: any;
  error?: string;
  processingTime: number;
  status: 'completed' | 'failed' | 'cancelled' | 'timeout';
  completedAt: string;
  metadata?: any;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

export interface TaskProgress {
  taskId: string;
  status: 'pending' | 'processing' | 'completed' | 'failed';
  progress: number;
  currentStep?: string;
  estimatedTimeRemaining?: number;
  startedAt?: string;
  completedAt?: string;
}

export interface QueueStats {
  waiting: number;
  active: number;
  completed: number;
  failed: number;
  delayed: number;
  paused: number;
}

export interface BatchTaskRequest {
  tasks: AITask[];
  batchId: string;
  options?: {
    concurrency?: number;
    timeout?: number;
    onProgress?: (progress: BatchProgress) => void;
  };
}

export interface BatchProgress {
  batchId: string;
  total: number;
  completed: number;
  failed: number;
  processing: number;
  progress: number;
  results: Record<string, AITaskResult>;
}
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
src/services/ai/types/document.types.ts
// æ–‡æ¡£åˆ†æç›¸å…³ç±»å‹å®šä¹‰
export interface ExtractionResult {
  success: boolean;
  data: Record<string, any>;
  confidence: number;
  missingFields: string[];
  warnings: string[];
  processedAt: string;
  metadata?: {
    extractionTime: number;
    documentSize: number;
    fieldCoverage: number;
  };
}

export interface DocumentComparisonResult {
  similarityScore: number;
  differences: string[];
  aspects?: Record<string, { similarity: number; details: string }>;
  summary: string;
  metadata?: {
    comparisonTime: number;
    docASize: number;
    docBSize: number;
  };
}

export interface DocumentSummaryResult {
  summary: string;
  keyPoints?: string[];
  length: number;
  coverage: number;
  style: 'concise' | 'detailed' | 'bullet' | 'executive';
  metadata?: {
    generationTime: number;
    originalLength: number;
    compressionRatio: number;
  };
}

export interface DocumentClassificationResult {
  primaryCategory: string;
  confidence: number;
  allCategories: Array<{ category: string; confidence: number; reasoning: string }>;
  metadata?: {
    classificationTime: number;
    categoriesConsidered: number;
  };
}

export interface DocumentQualityAssessment {
  score: number;
  aspects: {
    clarity: number;
    coherence: number;
    completeness: number;
    conciseness: number;
    accuracy?: number;
    relevance?: number;
  };
  suggestions: string[];
  overallFeedback: string;
  metadata?: {
    assessmentTime: number;
    documentComplexity: 'low' | 'medium' | 'high';
  };
}

export interface ExtractionSchema {
  [key: string]: {
    description: string;
    type: 'string' | 'number' | 'date' | 'boolean' | 'array';
    required?: boolean;
    validation?: {
      pattern?: string;
      min?: number;
      max?: number;
      options?: string[];
    };
  };
}

export interface DocumentProcessingOptions {
  maxDocumentLength?: number;
  language?: string;
  format?: 'auto' | 'text' | 'markdown' | 'html';
  includeMetadata?: boolean;
  timeout?: number;
}
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
src/services/ai/types/recommendation.types.ts
// æ¨èæœåŠ¡ç›¸å…³ç±»å‹å®šä¹‰
export interface RecommendationItem {
  id: string;
  score: number;
  reason: string;
  metadata?: any;
  confidence?: number;
  categories?: string[];
  features?: Record<string, any>;
}

export interface RecommendationContext {
  userId: string;
  userPreferences?: Record<string, any>;
  demographics?: {
    age?: number;
    gender?: string;
    location?: string;
    interests?: string[];
  };
  currentContext?: string;
  exclusionList?: string[];
  diversity?: number;
  freshness?: number; // 0-1, æ¨èæ–°é¢–æ€§
  popularity?: number; // 0-1, è€ƒè™‘æµè¡Œåº¦
}

export interface UserInteraction {
  itemId: string;
  interactionType: 'view' | 'click' | 'purchase' | 'rating' | 'share' | 'save';
  timestamp: string;
  rating?: number;
  duration?: number;
  context?: any;
}

export interface ItemMetadata {
  id: string;
  title?: string;
  description?: string;
  categories?: string[];
  tags?: string[];
  features?: Record<string, any>;
  popularity?: number;
  freshness?: number;
  quality?: number;
}

export interface RecommendationResult {
  items: RecommendationItem[];
  context: RecommendationContext;
  metadata: {
    generationTime: number;
    totalItemsConsidered: number;
    algorithm: 'content_based' | 'collaborative' | 'hybrid' | 'ai_powered';
    diversityScore?: number;
    coverageScore?: number;
  };
  pagination?: {
    page: number;
    pageSize: number;
    total: number;
    hasMore: boolean;
  };
}

export interface SimilarItemsRequest {
  itemId: string;
  itemDetails: any;
  candidateItems: ItemMetadata[];
  options?: {
    count?: number;
    similarityDimensions?: string[];
    excludeOriginal?: boolean;
    minSimilarity?: number;
  };
}

export interface RecommendationStrategy {
  name: string;
  weight: number;
  description: string;
  parameters?: Record<string, any>;
}

export interface RecommendationEngineConfig {
  strategies: RecommendationStrategy[];
  fallbackStrategy?: string;
  maxRetries?: number;
  cacheTtl?: number;
  diversityPenalty?: number;
  freshnessBoost?: number;
}
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
src/services/ai/base.service.ts
import OpenAI from 'openai';
import { logger } from '../../config/logger';
import { TokenUsageMetrics } from '../../config/metrics';
import { AppError } from '../../utils/app-error';
import { ErrorCode } from '../../constants/error-codes';
import { redis } from '../../config/redis';

export abstract class BaseAIService {
  protected openai: OpenAI;
  protected defaultModel: string;
  protected requestTimeout: number;
  protected maxRetries: number;
  
  constructor(serviceName: string = 'BaseAIService') {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
      organization: process.env.OPENAI_ORGANIZATION,
      timeout: Number(process.env.OPENAI_TIMEOUT || 30000),
      maxRetries: Number(process.env.OPENAI_MAX_RETRIES || 3),
    });
    this.defaultModel = process.env.OPENAI_MODEL || 'gpt-4o';
    this.requestTimeout = Number(process.env.OPENAI_TIMEOUT || 30000);
    this.maxRetries = Number(process.env.OPENAI_MAX_RETRIES || 3);
    
    logger.info(`${serviceName} initialized`, {
      model: this.defaultModel,
      timeout: this.requestTimeout,
      maxRetries: this.maxRetries,
    });
  }
  
  // æ™ºèƒ½ç¼“å­˜æœºåˆ¶
  protected async withCache<T>(
    cacheKey: string,
    generator: () => Promise<T>,
    ttl: number = 3600, // é»˜è®¤1å°æ—¶
    options: { 
      useContentHash?: boolean; 
      content?: string;
      prefix?: string;
    } = {}
  ): Promise<T> {
    const { useContentHash = false, content, prefix = 'ai' } = options;
    
    let finalCacheKey = `${prefix}:${cacheKey}`;
    
    // å¦‚æœå¯ç”¨å†…å®¹å“ˆå¸Œï¼ŒåŸºäºå†…å®¹ç”Ÿæˆæ›´ç²¾ç¡®çš„ç¼“å­˜é”®
    if (useContentHash && content) {
      const crypto = await import('crypto');
      const contentHash = crypto
        .createHash('md5')
        .update(content)
        .digest('hex')
        .substring(0, 12);
      finalCacheKey = `${finalCacheKey}:${contentHash}`;
    }
    
    try {
      const cacheData = await redis.get(finalCacheKey);
      
      // ç¼“å­˜å‘½ä¸­
      if (cacheData) {
        try {
          const parsed = JSON.parse(cacheData);
          logger.debug('AI cache hit', { cacheKey: finalCacheKey });
          return parsed;
        } catch (error) {
          logger.warn('AI cache parse error', { 
            cacheKey: finalCacheKey, 
            error: error.message 
          });
        }
      }
    } catch (error) {
      logger.warn('Redis cache access error', { 
        cacheKey: finalCacheKey, 
        error: error.message 
      });
    }
    
    // ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œç”Ÿæˆ
    logger.debug('AI cache miss', { cacheKey: finalCacheKey });
    const result = await generator();
    
    // æ›´æ–°ç¼“å­˜
    try {
      await redis.setex(finalCacheKey, ttl, JSON.stringify(result));
      logger.debug('AI cache updated', { 
        cacheKey: finalCacheKey, 
        ttl 
      });
    } catch (error) {
      logger.warn('Failed to cache AI result', { 
        cacheKey: finalCacheKey, 
        error: error.message 
      });
    }
    
    return result;
  }
  
  // AIè¯·æ±‚é‡è¯•å°è£…
  protected async withRetry<T>(
    operation: () => Promise<T>,
    options: {
      retries?: number;
      initialDelay?: number;
      maxDelay?: number;
      backoffFactor?: number;
      context?: string;
    } = {}
  ): Promise<T> {
    const {
      retries = this.maxRetries,
      initialDelay = 1000,
      maxDelay = 30000,
      backoffFactor = 2,
      context = 'AI operation'
    } = options;
    
    let lastError: any;
    
    for (let attempt = 0; attempt <= retries; attempt++) {
      try {
        return await operation();
      } catch (error) {
        lastError = error;
        
        // åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•
        if (!this.isRetryable(error) || attempt === retries) {
          throw this.normalizeAIError(error, context);
        }
        
        // æŒ‡æ•°é€€é¿å»¶è¿Ÿï¼Œå¸¦éšæœºæŠ–åŠ¨
        const baseDelay = initialDelay * Math.pow(backoffFactor, attempt);
        const jitter = baseDelay * 0.1 * Math.random(); // 10% éšæœºæŠ–åŠ¨
        const delay = Math.min(baseDelay + jitter, maxDelay);
        
        logger.warn(`AI request failed, retrying (${attempt + 1}/${retries})`, {
          context,
          error: error.message,
          delay: Math.round(delay),
          status: error.status,
          code: error.code,
        });
        
        await new Promise(resolve => setTimeout(resolve, delay));
      }
    }
    
    throw this.normalizeAIError(lastError, context);
  }
  
  // åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•
  private isRetryable(error: any): boolean {
    // 429 Too Many Requests - é€Ÿç‡é™åˆ¶
    // 500, 502, 503, 504 - æœåŠ¡å™¨é”™è¯¯
    if (error.status && [429, 500, 502, 503, 504].includes(error.status)) {
      return true;
    }
    
    // ç½‘ç»œé”™è¯¯
    if (error.code === 'ETIMEDOUT' || error.code === 'ECONNRESET' || error.code === 'ECONNREFUSED') {
      return true;
    }
    
    // OpenAI API ç‰¹å®šé”™è¯¯
    if (error.code === 'insufficient_quota' || error.code === 'rate_limit_exceeded') {
      return true;
    }
    
    // è¶…æ—¶é”™è¯¯
    if (error.name === 'TimeoutError' || error.code === 'TIMEOUT') {
      return true;
    }
    
    return false;
  }
  
  // æ ‡å‡†åŒ– AI é”™è¯¯
  private normalizeAIError(error: any, context: string = 'AI operation'): AppError {
    if (error instanceof AppError) {
      return error;
    }
    
    // OpenAI API é”™è¯¯
    if (error.status === 429) {
      return new AppError(
        `${context}ï¼šæœåŠ¡é€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•`,
        429,
        ErrorCode.AI_RATE_LIMIT
      );
    }
    
    if (error.status === 401) {
      return new AppError(
        `${context}ï¼šæœåŠ¡è®¤è¯å¤±è´¥`,
        401,
        ErrorCode.AI_AUTH_ERROR
      );
    }
    
    if (error.status === 403) {
      return new AppError(
        `${context}ï¼šæœåŠ¡è®¿é—®è¢«æ‹’ç»`,
        403,
        ErrorCode.AI_ACCESS_DENIED
      );
    }
    
    // ç½‘ç»œè¶…æ—¶
    if (error.code === 'ETIMEDOUT' || error.name === 'TimeoutError') {
      return new AppError(
        `${context}ï¼šè¯·æ±‚è¶…æ—¶`,
        408,
        ErrorCode.AI_TIMEOUT
      );
    }
    
    // é…é¢ä¸è¶³
    if (error.code === 'insufficient_quota') {
      return new AppError(
        `${context}ï¼šæœåŠ¡é…é¢ä¸è¶³`,
        429,
        ErrorCode.AI_QUOTA_EXCEEDED
      );
    }
    
    // å…¶ä»–é”™è¯¯
    const message = error.message || 'æœªçŸ¥é”™è¯¯';
    return new AppError(
      `${context}å¤±è´¥: ${message}`,
      500,
      ErrorCode.AI_PROCESSING_ERROR
    );
  }
  
  // è®°å½• token ä½¿ç”¨æƒ…å†µ
  protected trackTokenUsage(
    model: string, 
    usage: { prompt_tokens: number; completion_tokens: number; total_tokens: number },
    metadata?: Record<string, any>
  ): void {
    TokenUsageMetrics.inc({
      model,
      type: 'prompt',
      count: usage.prompt_tokens,
      ...metadata,
    });
    
    TokenUsageMetrics.inc({
      model,
      type: 'completion',
      count: usage.completion_tokens,
      ...metadata,
    });
    
    TokenUsageMetrics.inc({
      model,
      type: 'total',
      count: usage.total_tokens,
      ...metadata,
    });
    
    // è®°å½•è¯¦ç»†æ—¥å¿—
    logger.debug('AI token usage tracked', {
      model,
      prompt_tokens: usage.prompt_tokens,
      completion_tokens: usage.completion_tokens,
      total_tokens: usage.total_tokens,
      estimatedCost: this.estimateCost(model, usage.prompt_tokens, usage.completion_tokens),
      ...metadata,
    });
  }
  
  // è®¡ç®—é¢„ä¼°æˆæœ¬
  protected estimateCost(
    model: string,
    promptTokens: number,
    completionTokens: number
  ): number {
    const costRates: Record<string, { input: number; output: number }> = {
      'gpt-4o': { input: 2.5, output: 10 }, // $2.5/1M input, $10/1M output
      'gpt-4': { input: 30, output: 60 },
      'gpt-3.5-turbo': { input: 0.5, output: 1.5 },
    };
    
    const rates = costRates[model] || costRates['gpt-4o'];
    const inputCost = (promptTokens / 1000000) * rates.input;
    const outputCost = (completionTokens / 1000000) * rates.output;
    
    return Number((inputCost + outputCost).toFixed(6));
  }
  
  // éªŒè¯ API å¯†é’¥
  protected validateApiKey(): void {
    if (!process.env.OPENAI_API_KEY) {
      throw new AppError(
        'OpenAI APIå¯†é’¥æœªé…ç½®',
        500,
        ErrorCode.AI_CONFIG_ERROR
      );
    }
  }
  
  // æ¸…ç†æ•æ„Ÿæ•°æ®
  protected sanitizeData(data: any): any {
    if (typeof data !== 'object' || data === null) {
      return data;
    }
    
    const sensitiveFields = ['password', 'token', 'secret', 'key', 'authorization'];
    const sanitized = { ...data };
    
    for (const key in sanitized) {
      if (sensitiveFields.some(field => key.toLowerCase().includes(field))) {
        sanitized[key] = '***REDACTED***';
      } else if (typeof sanitized[key] === 'object') {
        sanitized[key] = this.sanitizeData(sanitized[key]);
      }
    }
    
    return sanitized;
  }
}
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
src/services/ai/user-behavior.service.ts
import { BaseAIService } from './base.service';
import { logger } from '../../config/logger';
import { AppError } from '../../utils/app-error';
import { ErrorCode } from '../../constants/error-codes';

// ç”¨æˆ·è¡Œä¸ºåˆ†æç›¸å…³ç±»å‹
export interface UserBehaviorPattern {
  pattern: string;
  confidence: number;
  suggestedActions?: string[];
  details?: {
    frequency?: string;
    impact?: 'high' | 'medium' | 'low';
    trend?: 'increasing' | 'decreasing' | 'stable';
    riskLevel?: 'low' | 'medium' | 'high';
  };
}

export interface ChurnRiskAnalysis {
  risk: number;
  factors: string[];
  riskLevel: 'low' | 'medium' | 'high' | 'critical';
  recommendations: string[];
  nextBestAction?: string;
  confidence: number;
  metadata?: {
    analysisDate: string;
    dataPoints: number;
    modelVersion: string;
  };
}

export interface UserSegment {
  name: string;
  description: string;
  userCount: number;
  characteristics: string[];
  targetUsers: string[];
  engagementLevel: 'high' | 'medium' | 'low';
  potentialValue: number;
}

export class UserBehaviorAnalysisService extends BaseAIService {
  constructor() {
    super('UserBehaviorAnalysisService');
  }
  
  // åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼
  async analyzeUserBehavior(
    userId: string,
    behaviors: Array<{ action: string, timestamp: string, context?: any }>,
    options: { 
      maxBehaviors?: number; 
      includeSuggestions?: boolean;
      timeRange?: { start: string; end: string };
    } = {}
  ): Promise<UserBehaviorPattern[]> {
    const { maxBehaviors = 50, includeSuggestions = true, timeRange } = options;
    
    // ä¼˜åŒ–æç¤ºè¯ï¼Œå‡å°‘ token æ¶ˆè€—
    const timeRangeStr = timeRange 
      ? `æ—¶é—´èŒƒå›´: ${timeRange.start} è‡³ ${timeRange.end}\n`
      : '';
    
    const prompt = `
åˆ†æç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œè¯†åˆ«å…³é”®æ¨¡å¼å¹¶æä¾›æ´å¯Ÿã€‚è¯·ä¿æŒåˆ†æç®€æ´ã€å®ç”¨ã€‚

ç”¨æˆ·ID: ${userId}
${timeRangeStr}è¡Œä¸ºè®°å½•æ•°é‡: ${behaviors.length}
åˆ†ææ ·æœ¬: ${Math.min(behaviors.length, maxBehaviors)} æ¡è®°å½•

è¡Œä¸ºæ ·æœ¬:
${behaviors
  .slice(0, maxBehaviors)
  .map((b, i) => `${i + 1}. ${b.action} - ${new Date(b.timestamp).toLocaleDateString('zh-CN')}${b.context ? ` (${JSON.stringify(b.context)})` : ''}`)
  .join('\n')}

è¯·åˆ†æ:
1. ä¸»è¦è¡Œä¸ºæ¨¡å¼ï¼ˆå¦‚ä½¿ç”¨é¢‘ç‡ã€æ—¶é—´è§„å¾‹ã€åŠŸèƒ½åå¥½ç­‰ï¼‰
2. å¼‚å¸¸è¡Œä¸ºæˆ–å˜åŒ–è¶‹åŠ¿
3. ${includeSuggestions ? 'é’ˆå¯¹æ€§æ”¹è¿›å»ºè®®' : 'å…³é”®æ´å¯Ÿ'}

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œç»“æ„å¦‚ä¸‹:
{
  "patterns": [
    {
      "pattern": "æ¨¡å¼æè¿°",
      "confidence": 0.85,
      ${includeSuggestions ? '"suggestedActions": ["å»ºè®®1", "å»ºè®®2"],' : ''}
      "details": {
        "frequency": "æ¨¡å¼é¢‘ç‡",
        "impact": "ä¸šåŠ¡å½±å“",
        "trend": "å˜åŒ–è¶‹åŠ¿"
      }
    }
  ],
  "summary": "æ€»ä½“åˆ†ææ‘˜è¦",
  "keyInsights": ["å…³é”®æ´å¯Ÿ1", "å…³é”®æ´å¯Ÿ2"]
}
`;

    try {
      return await this.withRetry(async () => {
        const response = await this.openai.chat.completions.create({
          model: this.defaultModel,
          messages: [
            {
              role: 'system',
              content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”¨æˆ·è¡Œä¸ºåˆ†æå¸ˆã€‚è¯·æä¾›å‡†ç¡®ã€å®ç”¨çš„è¡Œä¸ºæ¨¡å¼åˆ†æï¼Œé¿å…è¿‡åº¦è§£è¯»ã€‚'
            },
            { role: 'user', content: prompt }
          ],
          temperature: 0.2,
          max_tokens: 2000,
          response_format: { type: 'json_object' },
        });
        
        // è®°å½•tokenä½¿ç”¨
        if (response.usage) {
          this.trackTokenUsage(this.defaultModel, response.usage, {
            userId,
            analysisType: 'behavior_analysis',
            behaviorCount: behaviors.length,
            timeRange: timeRange ? 'custom' : 'all',
          });
        }
        
        // è§£æå“åº”
        const content = response.choices[0]?.message?.content || '{}';
        try {
          const parsedResponse = JSON.parse(content);
          return parsedResponse.patterns || [];
        } catch (error) {
          logger.error('Failed to parse AI behavior analysis response', { 
            userId, 
            content: content.substring(0, 200),
            error: error.message 
          });
          return [];
        }
      }, { context: 'ç”¨æˆ·è¡Œä¸ºåˆ†æ' });
    } catch (error) {
      logger.error('User behavior analysis failed', { 
        userId, 
        error: error.message,
        behaviorCount: behaviors.length 
      });
      throw new AppError('ç”¨æˆ·è¡Œä¸ºåˆ†æå¤±è´¥', 500, ErrorCode.AI_PROCESSING_ERROR);
    }
  }
  
  // é¢„æµ‹ç”¨æˆ·æµå¤±é£é™© - å¢å¼ºç‰ˆæœ¬
  async predictChurnRisk(
    userId: string, 
    userData: any
  ): Promise<ChurnRiskAnalysis> {
    const cacheKey = `churn_risk:${userId}:${userData.lastActivityDate || 'unknown'}`;
    
    return this.withCache(cacheKey, async () => {
      const prompt = `
åŸºäºç”¨æˆ·æ•°æ®è¯„ä¼°æµå¤±é£é™©å¹¶æä¾›æ”¹å–„å»ºè®®ã€‚

ç”¨æˆ·èµ„æ–™:
- ç”¨æˆ·ID: ${userId}
- æ³¨å†Œæ—¶é—´: ${userData.registrationDate}
- æœ€åæ´»è·ƒ: ${userData.lastActivityDate}
- æ´»è·ƒé¢‘ç‡: ${userData.activityFrequency} æ¬¡/å‘¨
- ä½¿ç”¨åŠŸèƒ½: ${userData.usedFeatures?.join(', ') || 'æ— '}
- ç”¨æˆ·ç±»å‹: ${userData.customerType}
- ä»˜è´¹çŠ¶æ€: ${userData.subscriptionStatus || 'æœªçŸ¥'}
- å†å²é—®é¢˜: ${userData.issues?.length > 0 ? userData.issues.join('; ') : 'æ— '}
- æ»¡æ„åº¦è¯„åˆ†: ${userData.satisfactionScore || 'æœªè¯„åˆ†'}
- æ”¯æŒäº’åŠ¨: ${userData.supportInteractions || 0} æ¬¡

åˆ†æè¦æ±‚:
1. æµå¤±é£é™©è¯„åˆ† (0-100)
2. ä¸»è¦é£é™©å› ç´  (æŒ‰é‡è¦æ€§æ’åº)
3. é£é™©ç­‰çº§ (low/medium/high/critical)
4. å…·ä½“æ”¹å–„å»ºè®®
5. ä¸‹ä¸€æ­¥æœ€ä½³è¡ŒåŠ¨
6. åˆ†æç½®ä¿¡åº¦ (0-1)

è¯·è¿”å›JSONæ ¼å¼:
{
  "risk": 65,
  "factors": ["å› ç´ 1", "å› ç´ 2"],
  "riskLevel": "medium",
  "recommendations": ["å»ºè®®1", "å»ºè®®2"],
  "nextBestAction": "å…·ä½“è¡ŒåŠ¨å»ºè®®",
  "confidence": 0.85
}
`;

      try {
        return await this.withRetry(async () => {
          const response = await this.openai.chat.completions.create({
            model: this.defaultModel,
            messages: [
              {
                role: 'system',
                content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æˆ·æˆåŠŸåˆ†æå¸ˆã€‚åŸºäºæ•°æ®æä¾›å‡†ç¡®çš„æµå¤±é£é™©è¯„ä¼°å’Œå¯è¡Œçš„æ”¹å–„å»ºè®®ã€‚è€ƒè™‘ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼ã€äº’åŠ¨é¢‘ç‡å’Œæ»¡æ„åº¦ã€‚'
              },
              { role: 'user', content: prompt }
            ],
            temperature: 0.1,
            max_tokens: 1500,
            response_format: { type: 'json_object' },
          });
          
          // è®°å½•tokenä½¿ç”¨
          if (response.usage) {
            this.trackTokenUsage(this.defaultModel, response.usage, {
              userId,
              analysisType: 'churn_prediction',
              customerType: userData.customerType,
              dataCompleteness: this.calculateDataCompleteness(userData),
            });
          }
          
          // è§£æå“åº”
          const content = response.choices[0]?.message?.content || '{}';
          try {
            const result = JSON.parse(content);
            
            // éªŒè¯ç»“æœæ ¼å¼
            if (typeof result.risk !== 'number' || !Array.isArray(result.factors)) {
              throw new Error('Invalid response format');
            }
            
            // æ·»åŠ å…ƒæ•°æ®
            result.metadata = {
              analysisDate: new Date().toISOString(),
              dataPoints: Object.keys(userData).length,
              modelVersion: '1.0',
            };
            
            return result;
          } catch (error) {
            logger.error('Failed to parse churn prediction response', { 
              userId, 
              content: content.substring(0, 200),
              error: error.message 
            });
            
            // è¿”å›é»˜è®¤ç»“æœ
            return {
              risk: 50,
              factors: ['æ•°æ®è§£æé”™è¯¯'],
              riskLevel: 'medium' as const,
              recommendations: ['è¯·æ£€æŸ¥ç”¨æˆ·æ•°æ®å®Œæ•´æ€§'],
              nextBestAction: 'è”ç³»ç”¨æˆ·äº†è§£ä½¿ç”¨æƒ…å†µ',
              confidence: 0.3,
              metadata: {
                analysisDate: new Date().toISOString(),
                dataPoints: 0,
                modelVersion: 'fallback',
              }
            };
          }
        }, { context: 'æµå¤±é£é™©é¢„æµ‹' });
      } catch (error) {
        logger.error('Churn prediction failed', { 
          userId, 
          error: error.message 
        });
        throw new AppError('æµå¤±é¢„æµ‹åˆ†æå¤±è´¥', 500, ErrorCode.AI_PROCESSING_ERROR);
      }
    }, 24 * 60 * 60); // ç¼“å­˜24å°æ—¶
  }
  
  // ç”¨æˆ·åˆ†ç¾¤åˆ†æ
  async segmentUsers(
    users: Array<{ id: string; behaviors: any[]; attributes: any }>,
    segmentCount: number = 4
  ): Promise<{
    segments: UserSegment[];
    insights: string[];
    metadata: {
      totalUsers: number;
      segmentationMethod: 'behavioral' | 'demographic' | 'hybrid';
      qualityScore: number;
    };
  }> {
    const userSamples = users.slice(0, 50); // é™åˆ¶æ ·æœ¬æ•°é‡
    
    const prompt = `
åŸºäºä»¥ä¸‹ç”¨æˆ·æ•°æ®è¿›è¡Œåˆ†æåˆ†ç¾¤ï¼š

ç”¨æˆ·æ•°æ®æ ·æœ¬ (å…±${users.length}ä¸ªç”¨æˆ·ï¼Œå±•ç¤º${userSamples.length}ä¸ªæ ·æœ¬):
${userSamples.map((user, index) => `
ç”¨æˆ· ${index + 1} (ID: ${user.id}):
- å±æ€§: ${JSON.stringify(user.attributes)}
- å…³é”®è¡Œä¸º: ${user.behaviors.slice(0, 5).map(b => b.action).join(', ')}
`).join('\n')}

è¯·å°†ç”¨æˆ·åˆ†æˆ ${segmentCount} ä¸ªæœ‰æ„ä¹‰çš„ç¾¤ç»„ï¼Œæ¯ä¸ªç¾¤ç»„åŒ…å«ï¼š
1. ç¾¤ç»„åç§°
2. ç¾¤ç»„æè¿°
3. ç”¨æˆ·æ•°é‡ä¼°è®¡
4. ä¸»è¦ç‰¹å¾
5. å‚ä¸åº¦æ°´å¹³ (high/medium/low)
6. æ½œåœ¨ä»·å€¼è¯„ä¼°

åŒæ—¶æä¾›3-5ä¸ªå…³é”®ä¸šåŠ¡æ´å¯Ÿã€‚

è¿”å›JSONæ ¼å¼ã€‚
`;

    try {
      return await this.withRetry(async () => {
        const response = await this.openai.chat.completions.create({
          model: this.defaultModel,
          messages: [
            {
              role: 'system',
              content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”¨æˆ·åˆ†ç¾¤ä¸“å®¶ã€‚åŸºäºè¡Œä¸ºå’Œå±æ€§ç‰¹å¾åˆ›å»ºæœ‰æ„ä¹‰çš„ç”¨æˆ·åˆ†ç¾¤ï¼Œä¸ºæ¯ä¸ªåˆ†ç¾¤æä¾›æ¸…æ™°çš„æè¿°å’Œä»·å€¼è¯„ä¼°ã€‚'
            },
            { role: 'user', content: prompt }
          ],
          temperature: 0.3,
          max_tokens: 2500,
          response_format: { type: 'json_object' },
        });
        
        if (response.usage) {
          this.trackTokenUsage(this.defaultModel, response.usage, {
            analysisType: 'user_segmentation',
            userCount: users.length,
            segmentCount,
          });
        }
        
        const content = response.choices[0]?.message?.content || '{}';
        try {
          const result = JSON.parse(content);
          
          return {
            segments: result.segments || [],
            insights: result.insights || [],
            metadata: {
              totalUsers: users.length,
              segmentationMethod: 'behavioral',
              qualityScore: 0.8,
            }
          };
        } catch (error) {
          logger.error('Failed to parse user segmentation response', { 
            error: error.message,
            content: content.substring(0, 200)
          });
          
          return {
            segments: [],
            insights: ['åˆ†ç¾¤åˆ†æå¤±è´¥'],
            metadata: {
              totalUsers: users.length,
              segmentationMethod: 'behavioral',
              qualityScore: 0,
            }
          };
        }
      }, { context: 'ç”¨æˆ·åˆ†ç¾¤åˆ†æ' });
    } catch (error) {
      logger.error('User segmentation failed', { error: error.message });
      throw new AppError('ç”¨æˆ·åˆ†ç¾¤åˆ†æå¤±è´¥', 500, ErrorCode.AI_PROCESSING_ERROR);
    }
  }
  
  // è®¡ç®—æ•°æ®å®Œæ•´æ€§åˆ†æ•°
  private calculateDataCompleteness(userData: any): number {
    const requiredFields = [
      'registrationDate',
      'lastActivityDate', 
      'activityFrequency',
      'customerType'
    ];
    
    const presentFields = requiredFields.filter(field => 
      userData[field] !== undefined && userData[field] !== null
    );
    
    return presentFields.length / requiredFields.length;
  }
}
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
src/services/ai/queue.service.ts
import Bull from 'bull';
import { logger } from '../../config/logger';
import { AppError } from '../../utils/app-error';
import { ErrorCode } from '../../constants/error-codes';
import { 
  AITask, 
  AITaskResult, 
  AITaskType, 
  QueueStats,
  TaskProgress,
  BatchTaskRequest,
  BatchProgress
} from './types/ai-task.types';
import { UserBehaviorAnalysisService } from './user-behavior.service';
import { DocumentAnalysisService, RecommendationService } from './document-recommendation.service';

export class AIQueueService {
  private static instance: AIQueueService;
  private queue: Bull.Queue;
  private behaviorService: UserBehaviorAnalysisService;
  private documentService: DocumentAnalysisService;
  private recommendationService: RecommendationService;
  private initialized = false;
  private batchProgress: Map<string, BatchProgress> = new Map();

  private constructor() {
    this.queue = new Bull('ai-tasks', {
      redis: {
        host: process.env.REDIS_HOST || 'localhost',
        port: Number(process.env.REDIS_PORT || 6379),
        password: process.env.REDIS_PASSWORD,
        db: Number(process.env.REDIS_DB || 0),
      },
      defaultJobOptions: {
        attempts: 3,
        backoff: {
          type: 'exponential',
          delay: 5000,
        },
        removeOnComplete: 1000,
        removeOnFail: 500,
        timeout: 5 * 60 * 1000, // 5åˆ†é’Ÿè¶…æ—¶
      },
    });

    this.behaviorService = new UserBehaviorAnalysisService();
    this.documentService = new DocumentAnalysisService();
    this.recommendationService = new RecommendationService();
  }

  static getInstance(): AIQueueService {
    if (!AIQueueService.instance) {
      AIQueueService.instance = new AIQueueService();
    }
    return AIQueueService.instance;
  }

  initialize(): void {
    if (this.initialized) return;

    // æ³¨å†Œå¤„ç†å™¨
    this.queue.process('*', async (job) => {
      return await this.processTask(job.data);
    });

    // ç›‘å¬äº‹ä»¶
    this.queue.on('completed', (job, result) => {
      logger.info('AI task completed', {
        taskId: job.data.id,
        type: job.data.type,
        processingTime: job.finishedOn - job.processedOn,
      });
    });

    this.queue.on('failed', (job, error) => {
      logger.error('AI task failed', {
        taskId: job.data.id,
        type: job.data.type,
        error: error.message,
        attempt: job.attemptsMade,
      });
    });

    this.queue.on('stalled', (job) => {
      logger.warn('AI task stalled', { taskId: job.data.id });
    });

    this.queue.on('error', (error) => {
      logger.error('AI queue error', { error: error.message });
    });

    this.initialized = true;
    logger.info('AI queue service initialized');
  }

  async addTask(task: AITask): Promise<string> {
    if (!this.initialized) {
      this.initialize();
    }

    if (!task.id || !task.type) {
      throw new AppError('Task must have id and type', 400, ErrorCode.INVALID_INPUT);
    }

    const options: Bull.JobOptions = {
      jobId: task.id,
      priority: this.getPriorityValue(task.metadata?.priority),
      attempts: task.options?.attempts || 3,
      timeout: task.options?.timeout || 5 * 60 * 1000,
      delay: task.options?.delay,
      removeOnComplete: 1000,
      removeOnFail: 500,
    };

    await this.queue.add(task, options);
    logger.info('AI task added to queue', { taskId: task.id, type: task.type });

    return task.id;
  }

  async addBatchTasks(batchRequest: BatchTaskRequest): Promise<string> {
    const { tasks, batchId, options = {} } = batchRequest;
    
    // åˆå§‹åŒ–æ‰¹æ¬¡è¿›åº¦
    const batchProgress: BatchProgress = {
      batchId,
      total: tasks.length,
      completed: 0,
      failed: 0,
      processing: 0,
      progress: 0,
      results: {},
    };
    
    this.batchProgress.set(batchId, batchProgress);
    
    // æ·»åŠ æ‰€æœ‰ä»»åŠ¡åˆ°é˜Ÿåˆ—
    const addPromises = tasks.map(task => this.addTask(task));
    await Promise.all(addPromises);
    
    logger.info('Batch tasks added to queue', {
      batchId,
      taskCount: tasks.length,
      concurrency: options.concurrency,
    });
    
    return batchId;
  }

  async getTaskStatus(taskId: string): Promise<TaskProgress> {
    if (!this.initialized) {
      this.initialize();
    }

    const job = await this.queue.getJob(taskId);
    if (!job) {
      return { taskId, status: 'not_found', progress: 0 };
    }

    const state = await job.getState();
    const progress = job.progress();

    const statusMap: Record<string, TaskProgress['status']> = {
      'waiting': 'pending',
      'active': 'processing',
      'completed': 'completed',
      'failed': 'failed',
      'delayed': 'pending',
    };

    const taskProgress: TaskProgress = {
      taskId,
      status: statusMap[state] || 'pending',
      progress: progress || 0,
      startedAt: job.processedOn ? new Date(job.processedOn).toISOString() : undefined,
      completedAt: job.finishedOn ? new Date(job.finishedOn).toISOString() : undefined,
    };

    if (state === 'completed') {
      taskProgress.currentStep = 'completed';
    } else if (state === 'failed') {
      taskProgress.currentStep = 'error';
    }

    return taskProgress;
  }

  async getBatchProgress(batchId: string): Promise<BatchProgress | null> {
    return this.batchProgress.get(batchId) || null;
  }

  async getQueueStats(): Promise<QueueStats> {
    if (!this.initialized) {
      this.initialize();
    }

    const [waiting, active, completed, failed, delayed] = await Promise.all([
      this.queue.getWaitingCount(),
      this.queue.getActiveCount(),
      this.queue.getCompletedCount(),
      this.queue.getFailedCount(),
      this.queue.getDelayedCount(),
    ]);

    return {
      waiting,
      active,
      completed,
      failed,
      delayed,
      paused: 0, // Bull 4 ä¸­å·²ç§»é™¤ paused è®¡æ•°
    };
  }

  async cancelTask(taskId: string): Promise<boolean> {
    if (!this.initialized) {
      this.initialize();
    }

    const job = await this.queue.getJob(taskId);
    if (!job) {
      return false;
    }

    try {
      await job.remove();
      logger.info('AI task cancelled', { taskId });
      return true;
    } catch (error) {
      logger.error('Failed to cancel AI task', { taskId, error });
      return false;
    }
  }

  async cleanupCompletedTasks(olderThanHours: number = 24): Promise<number> {
    if (!this.initialized) {
      this.initialize();
    }

    try {
      // æ¸…ç†å®Œæˆçš„ä»»åŠ¡
      const completedCount = await this.queue.clean(
        olderThanHours * 60 * 60 * 1000,
        'completed'
      );

      // æ¸…ç†å¤±è´¥çš„ä»»åŠ¡
      const failedCount = await this.queue.clean(
        olderThanHours * 60 * 60 * 1000,
        'failed'
      );

      logger.info('Queue cleanup completed', {
        completedCleaned: completedCount,
        failedCleaned: failedCount,
        olderThanHours,
      });

      return completedCount + failedCount;
    } catch (error) {
      logger.error('Queue cleanup failed', { error: error.message });
      return 0;
    }
  }

  private async processTask(task: AITask): Promise<any> {
    const startTime = Date.now();
    logger.info('Processing AI task', { taskId: task.id, type: task.type });

    try {
      let result: any;

      switch (task.type) {
        case 'user_behavior_analysis':
          result = await this.behaviorService.analyzeUserBehavior(
            task.data.userId,
            task.data.behaviors,
            task.data.options
          );
          break;

        case 'churn_prediction':
          result = await this.behaviorService.predictChurnRisk(
            task.data.userId,
            task.data.userData
          );
          break;

        case 'document_analysis':
          result = await this.documentService.extractInformation(
            task.data.documentText,
            task.data.schema,
            task.data.options
          );
          break;

        case 'content_generation':
          result = await this.documentService.generateSummary(
            task.data.documentText,
            task.data.options
          );
          break;

        case 'sentiment_analysis':
          // å¯ä»¥è°ƒç”¨å…¶ä»–æœåŠ¡ï¼Œè¿™é‡Œæš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
          result = { sentiment: 'positive', confidence: 0.85 };
          break;

        case 'recommendation':
          result = await this.recommendationService.getPersonalizedRecommendations(
            task.data.context,
            task.data.userHistory,
            task.data.availableItems,
            task.data.count
          );
          break;

        case 'custom':
          result = await this.processCustomTask(task.data);
          break;

        default:
          throw new AppError(`Unsupported task type: ${task.type}`, 400, ErrorCode.INVALID_INPUT);
      }

      const processingTime = Date.now() - startTime;

      // å¤„ç†å›è°ƒ
      await this.handleCallback(task, {
        id: task.id,
        result,
        processingTime,
        status: 'completed',
        completedAt: new Date().toISOString(),
        metadata: task.metadata,
      });

      return result;
    } catch (error) {
      const processingTime = Date.now() - startTime;

      // å¤„ç†å›è°ƒï¼ˆé”™è¯¯æƒ…å†µï¼‰
      await this.handleCallback(task, {
        id: task.id,
        result: null,
        error: error instanceof Error ? error.message : String(error),
        processingTime,
        status: 'failed',
        completedAt: new Date().toISOString(),
        metadata: task.metadata,
      });

      throw error;
    }
  }

  private async processCustomTask(data: any): Promise<any> {
    // å®ç°è‡ªå®šä¹‰ä»»åŠ¡å¤„ç†é€»è¾‘
    // è¿™é‡Œå¯ä»¥æ ¹æ® data.instruction æ‰§è¡Œç‰¹å®šæ“ä½œ
    return { customResult: 'Custom processing completed' };
  }

  private async handleCallback(task: AITask, result: AITaskResult): Promise<void> {
    const { callback } = task;
    if (!callback) return;

    const callbackPromises = [];

    // HTTP å›è°ƒ
    if (callback.url) {
      callbackPromises.push(
        this.sendHttpCallback(callback.url, result, task.id, callback.headers)
      );
    }

    // äº‹ä»¶å›è°ƒï¼ˆå¯ä»¥æ‰©å±•ä¸º WebSocket æˆ–äº‹ä»¶æ€»çº¿ï¼‰
    if (callback.event) {
      callbackPromises.push(
        this.emitEventCallback(callback.event, result, task.id)
      );
    }

    if (callbackPromises.length > 0) {
      try {
        await Promise.allSettled(callbackPromises);
        logger.info('AI task callbacks completed', { taskId: task.id });
      } catch (error) {
        logger.error('Some AI task callbacks failed', { 
          taskId: task.id,
          error: error.message 
        });
      }
    }
  }

  private async sendHttpCallback(
    url: string, 
    result: AITaskResult, 
    taskId: string,
    headers?: Record<string, string>
  ): Promise<void> {
    try {
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'X-API-Key': process.env.CALLBACK_API_KEY || '',
          'X-Task-Id': taskId,
          'User-Agent': 'AI-Service/1.0',
          ...headers,
        },
        body: JSON.stringify(result),
        timeout: 10000, // 10ç§’è¶…æ—¶
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      logger.debug('HTTP callback sent successfully', { taskId, url });
    } catch (error) {
      logger.error('HTTP callback failed', { 
        taskId, 
        url,
        error: error.message 
      });
      throw error;
    }
  }

  private async emitEventCallback(
    event: string, 
    result: AITaskResult, 
    taskId: string
  ): Promise<void> {
    // å®ç°äº‹ä»¶å‘å°„é€»è¾‘ï¼Œä¾‹å¦‚é€šè¿‡ WebSocket æˆ–äº‹ä»¶æ€»çº¿
    // è¿™é‡Œä»…è®°å½•æ—¥å¿—
    logger.debug('Event callback emitted', { taskId, event, result: result.status });
  }

  private getPriorityValue(priority?: string): number {
    switch (priority) {
      case 'critical': return 1;
      case 'high': return 2;
      case 'normal': return 3;
      case 'low': return 4;
      default: return 3;
    }
  }

  async close(): Promise<void> {
    if (this.initialized) {
      await this.queue.close();
      this.initialized = false;
      logger.info('AI queue closed');
    }
  }
}
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
src/services/ai/document-recommendation.service.ts
import { BaseAIService } from './base.service';
import { logger } from '../../config/logger';
import { AppError } from '../../utils/app-error';
import { ErrorCode } from '../../constants/error-codes';
import {
  ExtractionResult,
  DocumentComparisonResult,
  DocumentSummaryResult,
  DocumentClassificationResult,
  DocumentQualityAssessment,
  ExtractionSchema,
  DocumentProcessingOptions,
} from './types/document.types';
import {
  RecommendationItem,
  RecommendationContext,
  UserInteraction,
  ItemMetadata,
  RecommendationResult,
  SimilarItemsRequest,
  RecommendationEngineConfig,
} from './types/recommendation.types';

export class DocumentAnalysisService extends BaseAIService {
  constructor() {
    super('DocumentAnalysisService');
  }

  // ä»æ–‡æ¡£æå–ç»“æ„åŒ–ä¿¡æ¯
  async extractInformation(
    documentText: string, 
    schema: ExtractionSchema,
    options: DocumentProcessingOptions = {}
  ): Promise<ExtractionResult> {
    const { 
      maxDocumentLength = 8000,
      includeMetadata = true,
      timeout = 30000,
    } = options;

    const schemaStr = Object.entries(schema)
      .map(([key, field]) => `- ${key}: ${field.description} (ç±»å‹: ${field.type}${field.required ? ', å¿…éœ€' : ''})`)
      .join('\n');

    const requiredFields = Object.entries(schema)
      .filter(([_, field]) => field.required)
      .map(([key]) => key);

    const requiredStr = requiredFields.length > 0 
      ? `\n\nå¿…é¡»åŒ…å«å­—æ®µ: ${requiredFields.join(', ')}`
      : '';

    const isTruncated = documentText.length > maxDocumentLength;
    const processedText = documentText.substring(0, maxDocumentLength);

    const prompt = `
è¯·ä»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ã€‚ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„å­—æ®µå’Œæ ¼å¼è¦æ±‚æ‰§è¡Œã€‚

æ–‡æ¡£å†…å®¹${isTruncated ? 'ï¼ˆå·²æˆªæ–­ï¼‰' : ''}:
${processedText}${isTruncated ? '\n\n[æ–‡æ¡£å†…å®¹å·²æˆªæ–­ï¼Œä»…åˆ†æå‰éƒ¨åˆ†å†…å®¹]' : ''}

æå–å­—æ®µå®šä¹‰:
${schemaStr}
${requiredStr}

æå–è¦æ±‚:
1. åªæå–æ˜ç¡®å‡ºç°åœ¨æ–‡æ¡£ä¸­çš„ä¿¡æ¯
2. å¯¹äºä¸ç¡®å®šçš„ä¿¡æ¯ï¼Œæ ‡è®°ä¸ºnullæˆ–ç©ºå­—ç¬¦ä¸²
3. ä¿æŒæ•°æ®æ ¼å¼çš„ä¸€è‡´æ€§
4. ä¸è¦æ¨æ–­æˆ–åˆ›é€ æ–‡æ¡£ä¸­ä¸å­˜åœ¨çš„ä¿¡æ¯
5. å¯¹äºæ•°å­—å’Œæ—¥æœŸï¼Œå°½é‡ä¿æŒåŸå§‹æ ¼å¼

è¯·è¿”å›JSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µ:
- extracted: åŒ…å«æ‰€æœ‰æå–å­—æ®µçš„å¯¹è±¡
- confidence: æ•´ä½“æå–ç½®ä¿¡åº¦ (0-1)
- notes: ä»»ä½•å¤„ç†è¯´æ˜æˆ–è­¦å‘Š

ç¤ºä¾‹:
{
  "extracted": {
    "field1": "value1",
    "field2": null
  },
  "confidence": 0.85,
  "notes": ["å­—æ®µXæœªæ‰¾åˆ°", "å­—æ®µYæ ¼å¼éœ€è¦éªŒè¯"]
}
`;

    const startTime = Date.now();

    try {
      return await this.withRetry(async () => {
        const response = await this.openai.chat.completions.create({
          model: this.defaultModel,
          messages: [
            {
              role: 'system',
              content: 'ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„ä¿¡æ¯æå–åŠ©æ‰‹ã€‚ä¸¥æ ¼åŸºäºæ–‡æ¡£å†…å®¹æå–ä¿¡æ¯ï¼Œä¸è¿›è¡Œæ¨æ–­æˆ–åˆ›é€ ã€‚å¯¹äºä¸ç¡®å®šçš„ä¿¡æ¯ï¼Œè¯·æ˜ç¡®æ ‡è®°ã€‚è¿”å›æ ¼å¼å¿…é¡»ä¸ºæœ‰æ•ˆçš„JSONã€‚'
            },
            { role: 'user', content: prompt }
          ],
          temperature: 0.1,
          max_tokens: 3000,
          response_format: { type: 'json_object' },
        }, { timeout });

        // è®°å½•tokenä½¿ç”¨
        if (response.usage) {
          this.trackTokenUsage(this.defaultModel, response.usage, {
            analysisType: 'document_extraction',
            documentLength: documentText.length,
            fieldCount: Object.keys(schema).length,
            isTruncated,
          });
        }

        // è§£æå“åº”
        const content = response.choices[0]?.message?.content || '{}';
        try {
          const parsedResponse = JSON.parse(content);
          const extractedData = parsedResponse.extracted || {};
          const confidence = parsedResponse.confidence || 0.5;
          const notes = parsedResponse.notes || [];

          // éªŒè¯å¿…éœ€å­—æ®µ
          const missingFields = requiredFields.filter(
            field => !extractedData[field] || extractedData[field] === ''
          );

          const processingTime = Date.now() - startTime;

          // æ„å»ºç»“æœ
          const result: ExtractionResult = {
            success: missingFields.length === 0,
            data: extractedData,
            confidence,
            missingFields,
            warnings: notes,
            processedAt: new Date().toISOString(),
          };

          if (includeMetadata) {
            result.metadata = {
              extractionTime: processingTime,
              documentSize: documentText.length,
              fieldCoverage: (Object.keys(extractedData).length / Object.keys(schema).length),
            };
          }

          logger.info('Document extraction completed', {
            success: result.success,
            confidence,
            extractedFields: Object.keys(extractedData).length,
            missingFields: missingFields.length,
            documentLength: documentText.length,
            processingTime,
          });

          return result;
        } catch (error) {
          logger.error('Failed to parse document extraction response', { 
            content: content.substring(0, 200),
            error: error.message 
          });

          return {
            success: false,
            data: {},
            confidence: 0,
            missingFields: requiredFields,
            warnings: ['å“åº”è§£æå¤±è´¥: ' + error.message],
            processedAt: new Date().toISOString(),
          };
        }
      }, { context: 'æ–‡æ¡£ä¿¡æ¯æå–' });
    } catch (error) {
      logger.error('Document extraction failed', { error: error.message });
      throw new AppError('æ–‡æ¡£ä¿¡æ¯æå–å¤±è´¥', 500, ErrorCode.AI_PROCESSING_ERROR);
    }
  }

  // æ–‡æ¡£ç›¸ä¼¼åº¦æ¯”è¾ƒ
  async compareDocuments(
    docA: string, 
    docB: string,
    options: {
      maxLengthPerDoc?: number;
      aspects?: string[];
      detailedAnalysis?: boolean;
    } = {}
  ): Promise<DocumentComparisonResult> {
    const { 
      maxLengthPerDoc = 4000, 
      aspects = ['ä¸»é¢˜', 'ç»“æ„', 'å…³é”®ä¿¡æ¯', 'é£æ ¼'],
      detailedAnalysis = false
    } = options;

    const isTruncatedA = docA.length > maxLengthPerDoc;
    const isTruncatedB = docB.length > maxLengthPerDoc;

    const aspectStr = aspects.map(aspect => `- ${aspect}`).join('\n');
    const detailedAnalysisStr = detailedAnalysis ? 
      `\n\nåŒæ—¶åˆ†æä»¥ä¸‹æ–¹é¢çš„ç›¸ä¼¼åº¦:\n${aspectStr}` : '';

    const prompt = `
åˆ†æä»¥ä¸‹ä¸¤ä¸ªæ–‡æ¡£çš„ç›¸ä¼¼åº¦ï¼Œè®¡ç®—ç™¾åˆ†æ¯”ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-100ï¼‰ï¼Œå¹¶åˆ—å‡ºä¸»è¦å·®å¼‚ã€‚

æ–‡æ¡£A${isTruncatedA ? 'ï¼ˆå·²æˆªæ–­ï¼‰' : ''}:
${docA.substring(0, maxLengthPerDoc)}${isTruncatedA ? '\n\n[æ–‡æ¡£Aå†…å®¹å·²æˆªæ–­]' : ''}

æ–‡æ¡£B${isTruncatedB ? 'ï¼ˆå·²æˆªæ–­ï¼‰' : ''}:
${docB.substring(0, maxLengthPerDoc)}${isTruncatedB ? '\n\n[æ–‡æ¡£Bå†…å®¹å·²æˆªæ–­]' : ''}
${detailedAnalysisStr}

è¯·è¿”å›JSONæ ¼å¼:
{
  "similarityScore": ç›¸ä¼¼åº¦ç™¾åˆ†æ¯”ï¼ˆ0-100ä¹‹é—´çš„æ•°å€¼ï¼‰,
  "differences": [ä¸»è¦å·®å¼‚ç‚¹çš„æ•°ç»„],
  ${detailedAnalysis ? `"aspects": {
    "ä¸»é¢˜": { "similarity": ç›¸ä¼¼åº¦åˆ†æ•°, "details": "è¯¦ç»†è¯´æ˜" },
    ...
  },` : ''}
  "summary": "ç›¸ä¼¼åº¦åˆ†ææ‘˜è¦"
}
`;

    try {
      return await this.withRetry(async () => {
        const response = await this.openai.chat.completions.create({
          model: this.defaultModel,
          messages: [
            {
              role: 'system',
              content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æä¸“å®¶ã€‚è¯·å‡†ç¡®è¯„ä¼°æ–‡æ¡£ç›¸ä¼¼åº¦ï¼Œæä¾›å…·ä½“çš„å·®å¼‚åˆ†æã€‚'
            },
            { role: 'user', content: prompt }
          ],
          temperature: 0.1,
          max_tokens: 2000,
          response_format: { type: 'json_object' },
        });

        // è®°å½•tokenä½¿ç”¨
        if (response.usage) {
          this.trackTokenUsage(this.defaultModel, response.usage, {
            analysisType: 'document_comparison',
            docALength: docA.length,
            docBLength: docB.length,
            detailedAnalysis,
          });
        }

        // è§£æå“åº”
        const content = response.choices[0]?.message?.content || '{}';
        try {
          const result = JSON.parse(content);

          // éªŒè¯å’Œæ ‡å‡†åŒ–ç»“æœ
          const similarityScore = Math.max(0, Math.min(100, Number(result.similarityScore) || 0));
          const differences = Array.isArray(result.differences) ? result.differences : [];
          const summary = result.summary || 'æ–‡æ¡£ç›¸ä¼¼åº¦åˆ†æå®Œæˆ';

          const finalResult: DocumentComparisonResult = {
            similarityScore,
            differences,
            summary,
          };

          if (detailedAnalysis && result.aspects) {
            finalResult.aspects = result.aspects;
          }

          logger.info('Document comparison completed', {
            similarityScore,
            differenceCount: differences.length,
            docALength: docA.length,
            docBLength: docB.length,
          });

          return finalResult;
        } catch (error) {
          logger.error('Failed to parse document comparison response', { 
            content: content.substring(0, 200),
            error: error.message 
          });

          return {
            similarityScore: 0,
            differences: ['è§£æå“åº”æ—¶å‡ºé”™'],
            summary: 'åˆ†æå¤±è´¥',
          };
        }
      }, { context: 'æ–‡æ¡£ç›¸ä¼¼åº¦æ¯”è¾ƒ' });
    } catch (error) {
      logger.error('Document comparison failed', { error: error.message });
      throw new AppError('æ–‡æ¡£æ¯”è¾ƒå¤±è´¥', 500, ErrorCode.AI_PROCESSING_ERROR);
    }
  }

  // æ–‡æ¡£æ‘˜è¦ç”Ÿæˆ
  async generateSummary(
    documentText: string,
    options: {
      maxLength?: number;
      style?: 'concise' | 'detailed' | 'bullet' | 'executive';
      focusAreas?: string[];
      includeKeyPoints?: boolean;
    } = {}
  ): Promise<DocumentSummaryResult> {
    const { 
      maxLength = 200, 
      style = 'concise',
      focusAreas = [],
      includeKeyPoints = true
    } = options;

    const styleMap = {
      concise: 'ç®€æ´æ˜äº†',
      detailed: 'è¯¦ç»†å®Œæ•´',
      bullet: 'è¦ç‚¹å½¢å¼',
      executive: 'æ‰§è¡Œæ‘˜è¦é£æ ¼'
    };

    const focusStr = focusAreas.length > 0 
      ? `\né‡ç‚¹å…³æ³¨: ${focusAreas.join(', ')}`
      : '';

    const isTruncated = documentText.length > 8000;
    const processedText = documentText.substring(0, 8000);

    const prompt = `
è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆæ‘˜è¦ã€‚

ç”Ÿæˆè¦æ±‚:
- æ‘˜è¦é£æ ¼: ${styleMap[style]}
- æœ€å¤§é•¿åº¦: ${maxLength} å­—
- ${includeKeyPoints ? 'åŒ…å«3-5ä¸ªå…³é”®è¦ç‚¹' : 'ä»…ç”Ÿæˆæ‘˜è¦æ–‡æœ¬'}
${focusStr}

æ–‡æ¡£å†…å®¹${isTruncated ? 'ï¼ˆå·²æˆªæ–­ï¼‰' : ''}:
${processedText}${isTruncated ? '\n\n[æ–‡æ¡£å†…å®¹å·²æˆªæ–­]' : ''}

è¯·è¿”å›JSONæ ¼å¼:
{
  "summary": "æ‘˜è¦å†…å®¹",
  ${includeKeyPoints ? `"keyPoints": ["è¦ç‚¹1", "è¦ç‚¹2", ...],` : ''}
  "length": æ‘˜è¦å­—æ•°,
  "coverage": æ–‡æ¡£å†…å®¹è¦†ç›–ç‡ä¼°è®¡(0-1)
}
`;

    try {
      return await this.withRetry(async () => {
        const response = await this.openai.chat.completions.create({
          model: this.defaultModel,
          messages: [
            {
              role: 'system',
              content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£æ€»ç»“ä¸“å®¶ã€‚è¯·ç”Ÿæˆå‡†ç¡®ã€æœ‰ç”¨çš„æ‘˜è¦ï¼Œé¿å…é—æ¼é‡è¦ä¿¡æ¯ã€‚'
            },
            { role: 'user', content: prompt }
          ],
          temperature: 0.3,
          max_tokens: maxLength * 3, // ä¸ºå…³é”®ç‚¹ç•™å‡ºç©ºé—´
          response_format: { type: 'json_object' },
        });

        // è®°å½•tokenä½¿ç”¨
        if (response.usage) {
          this.trackTokenUsage(this.defaultModel, response.usage, {
            analysisType: 'document_summary',
            documentLength: documentText.length,
            style,
            includeKeyPoints,
          });
        }

        // è§£æå“åº”
        const content = response.choices[0]?.message?.content || '{}';
        try {
          const result = JSON.parse(content);

          const summary = result.summary || '';
          const keyPoints = includeKeyPoints && Array.isArray(result.keyPoints) ? result.keyPoints : undefined;
          const length = Number(result.length) || summary.length;
          const coverage = Math.max(0, Math.min(1, Number(result.coverage) || 0.5));

          logger.info('Document summary generated', {
            summaryLength: length,
            keyPointsCount: keyPoints?.length || 0,
            coverage,
            style,
          });

          return {
            summary,
            keyPoints,
            length,
            coverage,
            style,
          };
        } catch (error) {
          logger.error('Failed to parse summary response', { 
            content: content.substring(0, 200),
            error: error.message 
          });

          // é™çº§å¤„ç†ï¼šç›´æ¥ä½¿ç”¨æ–‡æœ¬å†…å®¹
          const fallbackSummary = content.length > maxLength 
            ? content.substring(0, maxLength) + '...' 
            : content;

          return {
            summary: fallbackSummary,
            length: fallbackSummary.length,
            coverage: 0.3,
            style,
          };
        }
      }, { context: 'æ–‡æ¡£æ‘˜è¦ç”Ÿæˆ' });
    } catch (error) {
      logger.error('Document summary generation failed', { error: error.message });
      throw new AppError('æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå¤±è´¥', 500, ErrorCode.AI_PROCESSING_ERROR);
    }
  }

  // æ–‡æ¡£åˆ†ç±»
  async classifyDocument(
    documentText: string,
    categories: string[]
  ): Promise<DocumentClassificationResult> {
    const categoriesStr = categories.map(cat => `- ${cat}`).join('\n');
    const processedText = documentText.substring(0, 6000);

    const prompt = `
è¯·å°†ä»¥ä¸‹æ–‡æ¡£åˆ†ç±»åˆ°æœ€åˆé€‚çš„ç±»åˆ«ä¸­ã€‚

å¯ç”¨ç±»åˆ«:
${categoriesStr}

æ–‡æ¡£å†…å®¹:
${processedText}${documentText.length > 6000 ? '\n\n[æ–‡æ¡£å†…å®¹å·²æˆªæ–­]' : ''}

è¯·è¿”å›JSONæ ¼å¼:
{
  "primaryCategory": "ä¸»è¦ç±»åˆ«",
  "confidence": ç½®ä¿¡åº¦(0-1),
  "allCategories": [
    {
      "category": "ç±»åˆ«åç§°",
      "confidence": ç½®ä¿¡åº¦,
      "reasoning": "åˆ†ç±»ç†ç”±"
    },
    ...
  ]
}
`;

    try {
      return await this.withRetry(async () => {
        const response = await this.openai.chat.completions.create({
          model: this.defaultModel,
          messages: [
            {
              role: 'system',
              content: 'ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†ç±»ä¸“å®¶ã€‚è¯·åŸºäºæ–‡æ¡£å†…å®¹é€‰æ‹©æœ€åˆé€‚çš„ç±»åˆ«ï¼Œå¹¶ä¸ºæ¯ä¸ªç±»åˆ«æä¾›ç½®ä¿¡åº¦å’Œç†ç”±ã€‚'
            },
            { role: 'user', content: prompt }
          ],
          temperature: 0.1,
          max_tokens: 1500,
          response_format: { type: 'json_object' },
        });

        if (response.usage) {
          this.trackTokenUsage(this.defaultModel, response.usage, {
            analysisType: 'document_classification',
            categoryCount: categories.length,
          });
        }

        const content = response.choices[0]?.message?.content || '{}';
        try {
          const result = JSON.parse(content);
          return result;
        } catch (error) {
          logger.error('Failed to parse classification response', { 
            content: content.substring(0, 200),
            error: error.message 
          });
          return {
            primaryCategory: categories[0] || 'æœªçŸ¥',
            confidence: 0,
            allCategories: [],
          };
        }
      }, { context: 'æ–‡æ¡£åˆ†ç±»' });
    } catch (error) {
      logger.error('Document classification failed', { error: error.message });
      throw new AppError('æ–‡æ¡£åˆ†ç±»å¤±è´¥', 500, ErrorCode.AI_PROCESSING_ERROR);
    }
  }

  // æ–‡æ¡£è´¨é‡è¯„ä¼°
  async assessDocumentQuality(
    documentText: string
  ): Promise<DocumentQualityAssessment> {
    const processedText = documentText.substring(0, 5000);

    const prompt = `
è¯„ä¼°ä»¥ä¸‹æ–‡æ¡£çš„è´¨é‡ï¼Œä»å¤šä¸ªç»´åº¦è¿›è¡Œè¯„åˆ†å¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚

æ–‡æ¡£å†…å®¹:
${processedText}${documentText.length > 5000 ? '\n\n[æ–‡æ¡£å†…å®¹å·²æˆªæ–­]' : ''}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°:
1. æ¸…æ™°åº¦ (Clarity): è¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚
2. è¿è´¯æ€§ (Coherence): é€»è¾‘æ˜¯å¦è¿è´¯
3. å®Œæ•´æ€§ (Completeness): ä¿¡æ¯æ˜¯å¦å®Œæ•´
4. ç®€æ´æ€§ (Conciseness): è¡¨è¾¾æ˜¯å¦ç®€æ´
5. å‡†ç¡®æ€§ (Accuracy): ä¿¡æ¯æ˜¯å¦å‡†ç¡®
6. ç›¸å…³æ€§ (Relevance): å†…å®¹æ˜¯å¦ç›¸å…³

è¯·è¿”å›JSONæ ¼å¼:
{
  "score": æ€»ä½“è´¨é‡è¯„åˆ†(0-1),
  "aspects": {
    "clarity": æ¸…æ™°åº¦è¯„åˆ†,
    "coherence": è¿è´¯æ€§è¯„åˆ†,
    "completeness": å®Œæ•´æ€§è¯„åˆ†,
    "conciseness": ç®€æ´æ€§è¯„åˆ†,
    "accuracy": å‡†ç¡®æ€§è¯„åˆ†,
    "relevance": ç›¸å…³æ€§è¯„åˆ†
  },
  "suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"],
  "overallFeedback": "æ€»ä½“åé¦ˆ"
}
`;

    try {
      return await this.withRetry(async () => {
        const response = await this.openai.chat.completions.create({
          model: this.defaultModel,
          messages: [
            {
              role: 'system',
              content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·ä»å¤šä¸ªç»´åº¦å®¢è§‚è¯„ä¼°æ–‡æ¡£è´¨é‡ï¼Œå¹¶æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚'
            },
            { role: 'user', content: prompt }
          ],
          temperature: 0.2,
          max_tokens: 2000,
          response_format: { type: 'json_object' },
        });

        if (response.usage) {
          this.trackTokenUsage(this.defaultModel, response.usage, {
            analysisType: 'document_quality_assessment',
            documentLength: documentText.length,
          });
        }

        const content = response.choices[0]?.message?.content || '{}';
        try {
          const result = JSON.parse(content);
          
          // æ·»åŠ å…ƒæ•°æ®
          result.metadata = {
            assessmentTime: Date.now(),
            documentComplexity: documentText.length > 1000 ? 'high' : documentText.length > 500 ? 'medium' : 'low',
          };

          return result;
        } catch (error) {
          logger.error('Failed to parse quality assessment response', { 
            content: content.substring(0, 200),
            error: error.message 
          });
          
          return {
            score: 0.5,
            aspects: {
              clarity: 0.5,
              coherence: 0.5,
              completeness: 0.5,
              conciseness: 0.5,
            },
            suggestions: ['æ–‡æ¡£è´¨é‡è¯„ä¼°å¤±è´¥'],
            overallFeedback: 'è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯',
          };
        }
      }, { context: 'æ–‡æ¡£è´¨é‡è¯„ä¼°' });
    } catch (error) {
      logger.error('Document quality assessment failed', { error: error.message });
      throw new AppError('æ–‡æ¡£è´¨é‡è¯„ä¼°å¤±è´¥', 500, ErrorCode.AI_PROCESSING_ERROR);
    }
  }
}

export class RecommendationService extends BaseAIService {
  private engineConfig: RecommendationEngineConfig;

  constructor() {
    super('RecommendationService');
    this.engineConfig = {
      strategies: [
        {
          name: 'content_based',
          weight: 0.6,
          description: 'åŸºäºå†…å®¹ç‰¹å¾çš„æ¨è'
        },
        {
          name: 'collaborative',
          weight: 0.3,
          description: 'åŸºäºç”¨æˆ·è¡Œä¸ºçš„ååŒè¿‡æ»¤'
        },
        {
          name: 'popularity',
          weight: 0.1,
          description: 'åŸºäºæµè¡Œåº¦çš„æ¨è'
        }
      ],
      fallbackStrategy: 'popularity',
      maxRetries: 3,
      cacheTtl: 1800, // 30åˆ†é’Ÿ
      diversityPenalty: 0.2,
      freshnessBoost: 0.1
    };
  }

  // åŸºäºç”¨æˆ·å†å²æ•°æ®æ¨èå†…å®¹
  async getPersonalizedRecommendations(
    context: RecommendationContext,
    userHistory: UserInteraction[],
    availableItems: ItemMetadata[],
    count: number = 5
  ): Promise<RecommendationResult> {
    const cacheKey = `recommendations:${context.userId}:${JSON.stringify(context)}:${userHistory.length}`;

    return this.withCache(cacheKey, async () => {
      // ä¼˜åŒ–: ä½¿ç”¨æ™ºèƒ½é¢„è¿‡æ»¤
      const preFilteredItems = this.preFilterItems(availableItems, userHistory, context.exclusionList || []);

      if (preFilteredItems.length === 0) {
        logger.warn('No items available after pre-filtering', { userId: context.userId });
        return {
          items: [],
          context,
          metadata: {
            generationTime: 0,
            totalItemsConsidered: 0,
            algorithm: 'ai_powered',
          },
        };
      }

      const historyText = userHistory
        .slice(-25)
        .map(h => `- ${h.itemId} (${h.interactionType}${h.rating ? `, è¯„åˆ†: ${h.rating}` : ''}, ${new Date(h.timestamp).toLocaleDateString('zh-CN')})`)
        .join('\n');

      const itemsText = preFilteredItems
        .slice(0, 25)
        .map(item => `- ${item.id}: ${JSON.stringify(item.metadata)}`)
        .join('\n');

      const diversityHint = context.diversity !== undefined ? 
        `\næ³¨æ„æ¨èå¤šæ ·æ€§ï¼Œå¤šæ ·æ€§ç³»æ•°: ${context.diversity}` : '';

      const contextHint = context.currentContext ? 
        `\nå½“å‰ä¸Šä¸‹æ–‡: ${context.currentContext}` : '';

      const prompt = `
åŸºäºç”¨æˆ·å†å²è¡Œä¸ºå’Œåå¥½ï¼Œæ¨èæœ€ç›¸å…³çš„${count}ä¸ªé¡¹ç›®ã€‚

ç”¨æˆ·ID: ${context.userId}
${contextHint}

ç”¨æˆ·å†å²è¡Œä¸ºï¼ˆæœ€è¿‘${Math.min(userHistory.length, 25)}æ¡ï¼‰:
${historyText}

ç”¨æˆ·åå¥½:
${context.userPreferences ? JSON.stringify(context.userPreferences, null, 2) : 'æ— æ˜ç¡®åå¥½'}

å¯ç”¨é¡¹ç›®ï¼ˆå·²é¢„ç­›é€‰ï¼‰:
${itemsText}
${diversityHint}

æ¨èè¦æ±‚:
1. ä¸ºæ¯ä¸ªæ¨èé¡¹ç›®è¯„åˆ†(1-100)ï¼Œè€ƒè™‘ç›¸å…³æ€§ã€ç”¨æˆ·åå¥½å’Œå¤šæ ·æ€§
2. æä¾›å…·ä½“çš„æ¨èç†ç”±
3. é¿å…æ¨èç”¨æˆ·å·²ç»äº¤äº’è¿‡çš„é¡¹ç›®
4. ${context.exclusionList?.length ? `æ’é™¤ä»¥ä¸‹é¡¹ç›®: ${context.exclusionList.join(', ')}` : ''}

è¿”å›JSONæ ¼å¼:
{
  "recommendations": [
    {
      "id": "é¡¹ç›®ID",
      "score": æ¨èè¯„åˆ†(1-100),
      "reason": "å…·ä½“æ¨èç†ç”±",
      "metadata": { å¯é€‰é¢å¤–ä¿¡æ¯ }
    },
    ...
  ]
}
`;

      const startTime = Date.now();

      try {
        return await this.withRetry(async () => {
          const response = await this.openai.chat.completions.create({
            model: this.defaultModel,
            messages: [
              {
                role: 'system',
                content: 'ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ¨èç³»ç»Ÿã€‚åŸºäºç”¨æˆ·å†å²å’Œè¡Œä¸ºæ¨¡å¼æä¾›ä¸ªæ€§åŒ–æ¨èï¼Œè€ƒè™‘å¤šæ ·æ€§å’Œæ–°é¢–æ€§ã€‚'
              },
              { role: 'user', content: prompt }
            ],
            temperature: Math.max(0.1, context.diversity || 0.3),
            max_tokens: 2000,
            response_format: { type: 'json_object' },
          });

          // è®°å½•tokenä½¿ç”¨
          if (response.usage) {
            this.trackTokenUsage(this.defaultModel, response.usage, {
              analysisType: 'personalized_recommendations',
              userId: context.userId,
              itemCount: preFilteredItems.length,
              historyCount: userHistory.length,
            });
          }

          // è§£æå“åº”
          const content = response.choices[0]?.message?.content || '{}';
          try {
            const result = JSON.parse(content);
            const recommendations = Array.isArray(result.recommendations) ? result.recommendations : [];

            // åå¤„ç†ï¼šæ’åºå’Œé™åˆ¶æ•°é‡
            const sortedRecommendations = recommendations
              .sort((a, b) => b.score - a.score)
              .slice(0, count);

            const generationTime = Date.now() - startTime;

            logger.info('Recommendations generated', {
              userId: context.userId,
              recommendationCount: sortedRecommendations.length,
              topScore: sortedRecommendations[0]?.score || 0,
              generationTime,
            });

            return {
              items: sortedRecommendations,
              context,
              metadata: {
                generationTime,
                totalItemsConsidered: preFilteredItems.length,
                algorithm: 'ai_powered',
                diversityScore: context.diversity,
              },
            };
          } catch (error) {
            logger.error('Failed to parse recommendations response', { 
              content: content.substring(0, 200),
              error: error.message 
            });
            return {
              items: [],
              context,
              metadata: {
                generationTime: Date.now() - startTime,
                totalItemsConsidered: 0,
                algorithm: 'ai_powered',
              },
            };
          }
        }, { context: 'ä¸ªæ€§åŒ–æ¨è' });
      } catch (error) {
        logger.error('Recommendation generation failed', { 
          userId: context.userId, 
          error: error.message 
        });
        throw new AppError('æ¨èç”Ÿæˆå¤±è´¥', 500, ErrorCode.AI_PROCESSING_ERROR);
      }
    }, this.engineConfig.cacheTtl);
  }

  // ç›¸ä¼¼é¡¹ç›®æŸ¥æ‰¾
  async findSimilarItems(
    request: SimilarItemsRequest
  ): Promise<RecommendationItem[]> {
    const { 
      itemId,
      itemDetails,
      candidateItems,
      options = {}
    } = request;

    const { 
      count = 5, 
      similarityDimensions = ['å†…å®¹', 'é£æ ¼', 'ä¸»é¢˜', 'å¤æ‚åº¦'],
      excludeOriginal = true,
      minSimilarity = 0
    } = options;

    const cacheKey = `similar_items:${itemId}:${JSON.stringify(candidateItems.map(i => i.id))}`;

    return this.withCache(cacheKey, async () => {
      const filteredCandidates = excludeOriginal 
        ? candidateItems.filter(item => item.id !== itemId)
        : candidateItems;

      if (filteredCandidates.length === 0) {
        return [];
      }

      const candidatesText = filteredCandidates
        .slice(0, 20)
        .map(item => `- ${item.id}: ${JSON.stringify(item.details)}`)
        .join('\n');

      const dimensionsStr = similarityDimensions.map(dim => `- ${dim}`).join('\n');

      const prompt = `
ä¸ºç»™å®šé¡¹ç›®æ‰¾å‡º${count}ä¸ªæœ€ç›¸ä¼¼çš„é¡¹ç›®ã€‚

ç›®æ ‡é¡¹ç›®:
ID: ${itemId}
è¯¦æƒ…: ${JSON.stringify(itemDetails)}

å€™é€‰é¡¹ç›®:
${candidatesText}

ç›¸ä¼¼åº¦è¯„ä¼°ç»´åº¦:
${dimensionsStr}

è¦æ±‚:
1. ä¸ºæ¯ä¸ªç›¸ä¼¼é¡¹è¯„åˆ†(1-100)ï¼Œè€ƒè™‘å¤šä¸ªç»´åº¦çš„ç»¼åˆç›¸ä¼¼åº¦
2. ç®€è¦è¯´æ˜ç›¸ä¼¼åŸå› ï¼Œå¯ä»¥å¼•ç”¨å…·ä½“ç»´åº¦
3. è€ƒè™‘è¡¨é¢ç‰¹å¾å’Œæ·±å±‚ç‰¹å¾çš„ç›¸ä¼¼æ€§

è¿”å›JSONæ ¼å¼:
{
  "similarItems": [
    {
      "id": "é¡¹ç›®ID",
      "score": ç›¸ä¼¼åº¦è¯„åˆ†(1-100),
      "reason": "ç›¸ä¼¼åŸå› ï¼Œå¯è¯´æ˜åœ¨å“ªäº›ç»´åº¦ç›¸ä¼¼"
    },
    ...
  ]
}
`;

      try {
        return await this.withRetry(async () => {
          const response = await this.openai.chat.completions.create({
            model: this.defaultModel,
            messages: [
              {
                role: 'system',
                content: 'ä½ æ˜¯ä¸€ä¸ªç›¸ä¼¼åº¦åˆ†æä¸“å®¶ã€‚è¯·ä»å¤šä¸ªç»´åº¦è¯„ä¼°é¡¹ç›®ç›¸ä¼¼æ€§ï¼Œæä¾›å‡†ç¡®çš„ç›¸ä¼¼åº¦è¯„åˆ†å’Œç†ç”±ã€‚'
              },
              { role: 'user', content: prompt }
            ],
            temperature: 0.1,
            max_tokens: 1500,
            response_format: { type: 'json_object' },
          });

          // è®°å½•tokenä½¿ç”¨
          if (response.usage) {
            this.trackTokenUsage(this.defaultModel, response.usage, {
              analysisType: 'similar_items',
              targetItem: itemId,
              candidateCount: filteredCandidates.length,
            });
          }

          // è§£æå“åº”
          const content = response.choices[0]?.message?.content || '{}';
          try {
            const result = JSON.parse(content);
            const similarItems = Array.isArray(result.similarItems) ? result.similarItems : [];

            // æ’åºã€è¿‡æ»¤å’Œé™åˆ¶æ•°é‡
            return similarItems
              .filter(item => item.score >= minSimilarity)
              .sort((a, b) => b.score - a.score)
              .slice(0, count);
          } catch (error) {
            logger.error('Failed to parse similar items response', { 
              content: content.substring(0, 200),
              error: error.message 
            });
            return [];
          }
        }, { context: 'ç›¸ä¼¼é¡¹ç›®æŸ¥æ‰¾' });
      } catch (error) {
        logger.error('Similar items search failed', { itemId, error: error.message });
        throw new AppError('ç›¸ä¼¼é¡¹ç›®æŸ¥æ‰¾å¤±è´¥', 500, ErrorCode.AI_PROCESSING_ERROR);
      }
    }, 60 * 60); // ç¼“å­˜1å°æ—¶
  }

  // åŸºäºå†…å®¹çš„æ¨èï¼ˆæ— ç”¨æˆ·å†å²ï¼‰
  async getContentBasedRecommendations(
    userProfile: { preferences?: any; demographics?: any },
    availableItems: ItemMetadata[],
    count: number = 5
  ): Promise<RecommendationResult> {
    const itemsText = availableItems
      .slice(0, 20)
      .map(item => `- ${item.id}: ${JSON.stringify(item.metadata)}`)
      .join('\n');

    const prompt = `
åŸºäºç”¨æˆ·ç”»åƒå’Œå†…å®¹ç‰¹å¾ï¼Œæ¨èæœ€ç›¸å…³çš„${count}ä¸ªé¡¹ç›®ã€‚

ç”¨æˆ·ç”»åƒ:
${JSON.stringify(userProfile, null, 2)}

å¯ç”¨é¡¹ç›®:
${itemsText}

æ¨èè¦æ±‚:
1. åŸºäºå†…å®¹ç‰¹å¾ä¸ç”¨æˆ·ç”»åƒçš„åŒ¹é…åº¦è¿›è¡Œæ¨è
2. ä¸ºæ¯ä¸ªé¡¹ç›®è¯„åˆ†(1-100)
3. è¯´æ˜æ¨èç†ç”±ï¼Œé‡ç‚¹è¯´æ˜å†…å®¹ç‰¹å¾å¦‚ä½•åŒ¹é…ç”¨æˆ·ç”»åƒ

è¿”å›JSONæ ¼å¼:
{
  "recommendations": [
    {
      "id": "é¡¹ç›®ID",
      "score": åŒ¹é…åº¦è¯„åˆ†(1-100),
      "reason": "æ¨èç†ç”±"
    },
    ...
  ]
}
`;

    const startTime = Date.now();

    try {
      return await this.withRetry(async () => {
        const response = await this.openai.chat.completions.create({
          model: this.defaultModel,
          messages: [
            {
              role: 'system',
              content: 'ä½ æ˜¯ä¸€ä¸ªå†…å®¹æ¨èä¸“å®¶ã€‚åŸºäºç”¨æˆ·ç”»åƒå’Œé¡¹ç›®å†…å®¹çš„åŒ¹é…åº¦è¿›è¡Œæ¨èï¼Œæä¾›å‡†ç¡®çš„è¯„åˆ†å’Œç†ç”±ã€‚'
            },
            { role: 'user', content: prompt }
          ],
          temperature: 0.2,
          max_tokens: 1500,
          response_format: { type: 'json_object' },
        });

        if (response.usage) {
          this.trackTokenUsage(this.defaultModel, response.usage, {
            analysisType: 'content_based_recommendations',
            itemCount: availableItems.length,
          });
        }

        const content = response.choices[0]?.message?.content || '{}';
        try {
          const result = JSON.parse(content);
          const recommendations = Array.isArray(result.recommendations) 
            ? result.recommendations.slice(0, count) 
            : [];

          return {
            items: recommendations,
            context: { userId: 'anonymous' },
            metadata: {
              generationTime: Date.now() - startTime,
              totalItemsConsidered: availableItems.length,
              algorithm: 'content_based',
            },
          };
        } catch (error) {
          logger.error('Failed to parse content-based recommendations', { 
            content: content.substring(0, 200),
            error: error.message 
          });
          return {
            items: [],
            context: { userId: 'anonymous' },
            metadata: {
              generationTime: Date.now() - startTime,
              totalItemsConsidered: 0,
              algorithm: 'content_based',
            },
          };
        }
      }, { context: 'åŸºäºå†…å®¹çš„æ¨è' });
    } catch (error) {
      logger.error('Content-based recommendations failed', { error: error.message });
      throw new AppError('åŸºäºå†…å®¹çš„æ¨èå¤±è´¥', 500, ErrorCode.AI_PROCESSING_ERROR);
    }
  }

  // æ›´æ–°æ¨èå¼•æ“é…ç½®
  updateEngineConfig(config: Partial<RecommendationEngineConfig>): void {
    this.engineConfig = { ...this.engineConfig, ...config };
    logger.info('Recommendation engine config updated', { config });
  }

  // è·å–æ¨èå¼•æ“é…ç½®
  getEngineConfig(): RecommendationEngineConfig {
    return { ...this.engineConfig };
  }

  // è¾…åŠ©æ–¹æ³•ï¼šæ™ºèƒ½é¢„è¿‡æ»¤
  private preFilterItems(
    availableItems: ItemMetadata[],
    userHistory: UserInteraction[],
    exclusionList: string[]
  ): ItemMetadata[] {
    const interactedIds = new Set(userHistory.map(h => h.itemId));
    const excludedIds = new Set(exclusionList);

    return availableItems.filter(item => {
      // æ’é™¤å·²äº¤äº’å’Œæ˜ç¡®æ’é™¤çš„é¡¹ç›®
      if (interactedIds.has(item.id) || excludedIds.has(item.id)) {
        return false;
      }

      // å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šé¢„è¿‡æ»¤é€»è¾‘
      // æ¯”å¦‚åŸºäºç®€å•çš„å†…å®¹åŒ¹é…ã€æµè¡Œåº¦ç­‰

      return true;
    });
  }
}

// æœåŠ¡ç®¡ç†å™¨
export class AIServiceManager {
  private static instance: AIServiceManager;
  private services: Map<string, any> = new Map();

  static getInstance(): AIServiceManager {
    if (!AIServiceManager.instance) {
      AIServiceManager.instance = new AIServiceManager();
    }
    return AIServiceManager.instance;
  }

  initialize(): void {
    // åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡
    this.services.set('document', new DocumentAnalysisService());
    this.services.set('recommendation', new RecommendationService());
    this.services.set('behavior', new UserBehaviorAnalysisService());

    // åˆå§‹åŒ–é˜Ÿåˆ—æœåŠ¡
    const queueService = AIQueueService.getInstance();
    queueService.initialize();

    logger.info('AI services initialized successfully');
  }

  getService<T>(name: string): T {
    const service = this.services.get(name);
    if (!service) {
      throw new AppError(`AI service not found: ${name}`, 500, ErrorCode.SERVICE_UNAVAILABLE);
    }
    return service;
  }

  async shutdown(): Promise<void> {
    const queueService = AIQueueService.getInstance();
    await queueService.close();
    logger.info('AI services shutdown completed');
  }
}
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
src/services/ai/index.ts
// ç»Ÿä¸€å¯¼å‡ºæ‰€æœ‰ AI æœåŠ¡
export * from './base.service';
export * from './user-behavior.service';
export * from './queue.service';
export * from './document-recommendation.service';

// å¯¼å‡ºç±»å‹
export * from './types/ai-task.types';
export * from './types/document.types';
export * from './types/recommendation.types';

// é»˜è®¤å¯¼å‡ºæœåŠ¡ç®¡ç†å™¨
export { AIServiceManager } from './document-recommendation.service';
export default AIServiceManager;
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
### ğŸ”§ æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
1. BaseAIServiceÂ - æä¾›é€šç”¨AIåŠŸèƒ½
    - æ™ºèƒ½ç¼“å­˜æœºåˆ¶
    - é‡è¯•ç­–ç•¥
    - é”™è¯¯å¤„ç†
    - Tokenä½¿ç”¨è·Ÿè¸ª
    - æˆæœ¬ä¼°ç®—
2. UserBehaviorAnalysisServiceÂ - ç”¨æˆ·è¡Œä¸ºåˆ†æ
    - è¡Œä¸ºæ¨¡å¼è¯†åˆ«
    - æµå¤±é£é™©é¢„æµ‹
    - ç”¨æˆ·åˆ†ç¾¤åˆ†æ
3. DocumentAnalysisServiceÂ - æ–‡æ¡£å¤„ç†
    - ç»“æ„åŒ–ä¿¡æ¯æå–
    - æ–‡æ¡£ç›¸ä¼¼åº¦æ¯”è¾ƒ
    - æ™ºèƒ½æ‘˜è¦ç”Ÿæˆ
    - æ–‡æ¡£åˆ†ç±»
    - è´¨é‡è¯„ä¼°
4. RecommendationServiceÂ - æ™ºèƒ½æ¨è
    - ä¸ªæ€§åŒ–æ¨è
    - ç›¸ä¼¼é¡¹ç›®æŸ¥æ‰¾
    - åŸºäºå†…å®¹çš„æ¨è
    - æ¨èå¼•æ“é…ç½®
5. AIQueueServiceÂ - ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†
    - å¼‚æ­¥ä»»åŠ¡å¤„ç†
    - æ‰¹é‡ä»»åŠ¡æ”¯æŒ
    - è¿›åº¦è·Ÿè¸ª
    - å›è°ƒå¤„ç†
### ğŸš€ ä½¿ç”¨ç¤ºä¾‹
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
import AIServiceManager, { 
  DocumentAnalysisService, 
  RecommendationService,
  AIQueueService 
} from '@/services/ai';

// åˆå§‹åŒ–æœåŠ¡
const serviceManager = AIServiceManager.getInstance();
serviceManager.initialize();

// ä½¿ç”¨æ–‡æ¡£åˆ†ææœåŠ¡
const documentService = serviceManager.getService<DocumentAnalysisService>('document');
const extractionResult = await documentService.extractInformation(documentText, schema);

// ä½¿ç”¨æ¨èæœåŠ¡
const recommendationService = serviceManager.getService<RecommendationService>('recommendation');
const recommendations = await recommendationService.getPersonalizedRecommendations(
  context, userHistory, availableItems
);

// ä½¿ç”¨é˜Ÿåˆ—æœåŠ¡
const queueService = AIQueueService.getInstance();
const taskId = await queueService.addTask({
  id: 'task-1',
  type: 'document_analysis',
  data: { documentText, schema }
});
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
å®Œæ•´çš„ä¼ä¸šçº§ AI æœåŠ¡è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é«˜å¯ç”¨æ€§ã€å¯æ‰©å±•æ€§å’Œæ˜“ç»´æŠ¤æ€§ã€‚